<!doctype html><html><head><title>Accelerated Feedback Loops when Developing for Kubernetes with Conftest // Plex Engineering</title><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:title" content="Accelerated Feedback Loops when Developing for Kubernetes with Conftest"><meta property="og:description" content="Blog posts written by engineers at Plex Systems"><meta property="og:type" content="website"><meta property="og:locale" content="en"><meta property="og:url" content="https://plexsystems.github.io/posts/kubernetes-policy-conftest/"><meta property="og:image" content="https://engineering.plex.com/images/plexsocial.png"><link rel="shortcut icon" href=/favicon.ico?><link href=https://plexsystems.github.io/webfonts/ptserif/main.css rel=stylesheet type=text/css><link href=https://plexsystems.github.io/webfonts/source-code-pro/main.css rel=stylesheet type=text/css><link href=https://plexsystems.github.io/webfonts/helvetica/main.css rel=stylesheet type=text/css><link rel=stylesheet href=https://plexsystems.github.io/css/style.css><link rel=stylesheet href=https://plexsystems.github.io/css/custom.css><meta name=generator content="Hugo 0.71.1"></head><body><div id=container><header id=header><div id=header-outer class=outer><div id=header-inner class=inner><div class=logo-display><a class=logo href=https://plexsystems.github.io/></a><a class=logo-text href=https://plexsystems.github.io/>Engineering</a></div><div class=icons-display><a href=//github.com/plexsystems><img class=github-img src=/images/github.png alt=github></a></div></div></header><section id=main class=outer><article class="article article-type-post" itemscope itemprop=blogPost><div class=article-inner><header class=article-header><h1 class=article-title itemprop=name>Accelerated Feedback Loops when Developing for Kubernetes with Conftest</h1></header><div class=article-meta><img alt="John Reese" class=article-author-img src="https://avatars2.githubusercontent.com/jpreese?v=3&s=40">
<span class=article-author><a href=//github.com/jpreese>John Reese</a></span>
<img class=calendar-img src=/images/calendar.png alt=calendar>
<span class=article-date><time datetime=05-20-2020 itemprop=datePublished>May 23, 2020</time></span>
//
<span class=article-categories><a class=article-category-link href=https://plexsystems.github.io/categories/kubernetes>kubernetes</a>
<a class=article-category-link href=https://plexsystems.github.io/categories/policy>policy</a>
<a class=article-category-link href=https://plexsystems.github.io/categories/security>security</a>
<a class=article-category-link href=https://plexsystems.github.io/categories/code-reviews>code reviews</a></span></div><div class=article-entry itemprop=articleBody><p><img src=/images/kubernetes-policy-conftest/featured.jpg><br><p>The feedback loop when deploying to Kubernetes can be quite slow. Not only does the YAML need to be syntactically correct, but we need to ask ourselves:</p><p>Is the API version of our resource definition compatible with the version of Kubernetes that it is being deployed to? Kubernetes is constantly evolving, and over time, <a href=https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api>deprecates older APIs</a> in favor of newer ones. A deployment definition may successfully apply on one version of Kubernetes, but not another.</p><p>Are the resources that depend on one another configured properly? For example, when creating a <a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a>, you can specify <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/>selectors</a> for which Pods the traffic should ultimately be routed to. In the event that a Service is configured with selectors, but is unable to find any Pods, this mistake would not be known until the Service is deployed to the cluster. What makes this case even more challenging is that this configuration is technically valid but would require additional testing to verify if the traffic is flowing as expected.</p><p><a href=https://github.com/kubernetes-sigs/kind>Kind</a> aims to help solve a lot of these concerns by allowing developers to spin up a Kubernetes cluster on their local machine, verify their changes, and tear it down with ease. However, it can still take a fair amount of time to bring up a Kind cluster, apply the manifests, and test the outcome.</p><p>While these concerns can be caught relatively early, there are additional considerations, especially in the realm of security, that may not be immediately obvious and could go unnoticed until they become a <a href=https://unit42.paloaltonetworks.com/non-root-containers-kubernetes-cve-2019-11245-care/>serious problem</a>.</p><p>To catch a lot of these problems ahead of time without the need of a Kubernetes cluster, including Kind, Plex validates all deployments to Kubernetes against policies. A lot of policies.</p><h2 id=an-example-policy>An example policy</h2><p>Most of our policies are written in <a href=https://www.openpolicyagent.org/docs/latest/policy-language/>Rego</a>, a policy language that is interpreted by the <a href=https://www.openpolicyagent.org/>Open Policy Agent</a> (OPA). To better understand Rego, and how we can leverage it to write policies for Kubernetes, consider the following scenario:</p><p>In our cluster, we want to be able to quickly identify which team owns a given Namespace. This can be useful for being able to notify teams about overutilization, cost reporting, problematic pods, and more. To accomplish this, we require that all namespaces must be created with an <code>owner</code> label.</p><p>To show how we can use Rego to validate Kubernetes manifests, let&rsquo;s create a namespace without an <code>owner</code> label:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#66d9ef>apiVersion</span>: v1
<span style=color:#66d9ef>kind</span>: Namespace
<span style=color:#66d9ef>metadata</span>:
  <span style=color:#66d9ef>name</span>: missinglabel
</code></pre></div><pre><code class=language-subtext data-lang=subtext>namespace.yaml
</code></pre><p>We&rsquo;ll also need a policy. A Rego policy to enforce this requirement would look like the following:</p><script type=application/javascript src=https://gist.github.com/jpreese/42a367e9cc01323931d863bba2777ad1.js></script><p><em>NOTE: The <code>input</code> prefix is special when writing policies. It refers to the input document which is one of the base documents provided by the OPA <a href=https://www.openpolicyagent.org/docs/latest/philosophy/#the-opa-document-model>document model</a>.</em></p><p>This policy defines a single <a href=https://www.openpolicyagent.org/docs/latest/policy-language/#rules>rule</a> called <code>violation</code>. While the order of the statements within the rule do not matter, it can be easier to understand how a rule is evaluated if expressed in this way.</p><p><strong>Line 04</strong> first evaluates if the input has a <code>kind</code> property and if its value is equal to Namespace. If the <code>kind</code> is not a Namespace, or there does not exist a <code>kind</code> property at all, the input will not be considered a violation. The example Namespace has both a <code>kind</code> property and has a value of &ldquo;Namespace&rdquo;, so the statement in the rule would return true.</p><p><strong>Line 05</strong> then checks to see if there exists a key named <code>owner</code> in the labels map within the manifests metadata. If there is a key named <code>owner</code>, then Namespace must have an owner label. In the example Namespace, there is not an <code>owner</code> label so this statement also returns true.</p><p><strong>Line 07</strong> is an assignment operation which will always return true by default.</p><p>After all of the statements have been evaluated, the rule itself can be evaluated. In order for a rule to be true, <em>all</em> of the statements inside of the rule must also be true. In this example, all of the statements returned true so the example Namespace would trigger the violation.</p><h2 id=validating-kubernetes-manifests-with-conftest>Validating Kubernetes manifests with Conftest</h2><p>It is important to note that the Open Policy Agent always expects <em>JSON</em> in order to evaluate policies. Kubernetes on the other hand, speaks YAML. At its core, the Open Policy Agent is just a policy engineâ€”it&rsquo;s intended to be generic and fit many use cases.</p><p><a href=https://github.com/open-policy-agent/conftest>Conftest</a> is a tool that focuses on the user experience when interacting with OPA. Most notably, it handles converting multiple file formats such as <code>.hcl</code>, <code>Dockerfile</code>, and even <code>yaml</code> into JSON so that it can be interpreted by OPA. At Plex, we use Conftest in many of our pipelines. And because Conftest is just a CLI that can be downloaded onto other developer&rsquo;s machines, many of our developers verify their manifests against our policies on their local environments before ever creating a pull request.</p><p>Here is an example of a policy that enforces resource constraints on all containers:</p><script type=application/javascript src=https://gist.github.com/jpreese/b02db173a52fdf2257dee5cab4b33ade.js></script><p>If we were to run Conftest against the <a href=https://github.com/kubernetes/ingress-nginx/blob/master/deploy/static/provider/cloud/deploy.yaml>nginx ingress controller</a> bundle, we would see that it fails our policy:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ conftest test bundle.yaml

FAIL - <span style=color:#f92672>(</span>ingress-nginx<span style=color:#f92672>)</span>: Container constraints must be specified.
</code></pre></div><pre><code class=language-subtext data-lang=subtext>terminal
</code></pre><p>We can then take the necessary steps to add the resource constraints into the Deployment so that our CI will allow the bundle onto our cluster.</p><h2 id=using-policy-bundles>Using policy bundles</h2><p>With Conftest, there&rsquo;s a lot of freedom in the ability to write our own policies, but there are a lot of <a href=https://www.conftest.dev/sharing/>bundles</a> that the community has written that we also leverage.</p><p>A policy bundle can be thought of as a collection of Rego policies that can be pulled from a remote source. A lot of best practices are generic enough that it wouldn&rsquo;t make sense for everyone to have to write and rewrite the same policies. While the concept of bundling and distributing Rego policies for Kubernetes is still quite new, there do exist a couple bundles that have provided immediate value to our pipelines.</p><h3 id=verify-api-compatibility-with-deprek8ion>Verify API compatibility with Deprek8ion</h3><p><a href=https://github.com/swade1987/deprek8ion>Deprek8ion</a> is a set of Rego policies that can be used to see if any of our resources are currently, or will be, deprecated in a given Kubernetes release.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ conftest pull github.com/swade1987/deprek8ion/policies -p policy/deprek8ion
$ conftest test bundle.yaml

WARN - ingress-nginx-admission: API admissionregistration.k8s.io/v1beta1
is deprecated in Kubernetes 1.19, use admissionregistration.k8s.io/v1 instead.
</code></pre></div><pre><code class=language-subtext data-lang=subtext>terminal
</code></pre><h3 id=find-security-concerns-with-kubesec>Find security concerns with Kubesec</h3><p><a href=https://kubesec.io/>Kubesec</a> is a set of Rego policies that can be used to see if any of our resources have any insecure configurations.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ conftest pull github.com/instrumenta/policies/kubernetes -p policy/kubesec
$ conftest test bundle.yaml

FAIL - Deployment ingress-nginx-controller does not drop all capabilities
FAIL - Deployment ingress-nginx-controller is not using a read only root filesystem
FAIL - Deployment ingress-nginx-controller allows privileged escalation
FAIL - Deployment ingress-nginx-controller is running as root
</code></pre></div><pre><code class=language-subtext data-lang=subtext>terminal
</code></pre><p>Conftest enables us run policies against multiple resources at onceâ€”it is simple, yet powerful. No matter where the Kubernetes manifests originate from, in house or from the open, we can automatically execute our policies to validate that they&rsquo;re compliant with our requirements. This approach allows us to automate our standards and security compliant concerns, freeing up developers to focus on other tasks.</p><h2 id=investing-in-a-future-with-policy>Investing in a future with policy</h2><p>The general-purpose approach that the Open Policy Agent has taken, and the user experience that Conftest provides, enables near unlimited use cases for policy-based validation. We&rsquo;ve invested heavily in using policy to validate our Kubernetes resources, but already have plans to leverage policy in other scenarios. Notably continuous Kubernetes cluster auditing with <a href=https://github.com/open-policy-agent/gatekeeper>Gatekeeper</a>, and infrastructure security compliance with <a href=https://github.com/fugue/regula>Regula</a>.</p><p>Policy-based validation is still relatively new to the Kubernetes ecosystem, but has already made a large impact for us and we&rsquo;re excited to see what&rsquo;s coming next in this space.</p></p></div></div><nav id=article-nav><a href=/posts/container-structure-test/ id=article-nav-newer class=article-nav-link-wrap><div class=article-nav-title><span>&lt;</span>&nbsp;
Testing Containers with Container Structure Test</div></a><a href=/posts/deploying-infrastructure-azure/ id=article-nav-older class=article-nav-link-wrap><div class=article-nav-title>Deploying Atlantis for Azure DevOps onto Kubernetes&nbsp;<span>></span></div></a></nav></article><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"plexengineering"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div><br><footer><div class=outer><div id=footer-note>All blog posts are written by engineers at Plex Systems. Our goal is to share our experiences in order to
promote openness and collaboration within the technical community.</div><div id=copyright>&copy; 2020 Plex Systems</div></div><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-170945889-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/tomorrow-night.min.css integrity="sha256-2wL88NKUqvJi/ExflDzkzUumjUM73mcK2gBvBBeLvTk=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js integrity="sha256-KbfTjB0WZ8vvXngdpJGY3Yp3xKk+tttbqClO11anCIU=" crossorigin=anonymous></script><script>hljs.initHighlightingOnLoad();</script><script></script></footer></body></html>